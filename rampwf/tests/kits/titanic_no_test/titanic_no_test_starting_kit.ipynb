{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Paris Saclay Center for Data Science](http://www.datascience-paris-saclay.fr)\n",
    "\n",
    "## [Titanic RAMP](http://www.ramp.studio/problems/titanic): survival prediction of Titanic passengers\n",
    "\n",
    "_Benoit Playe (Institut Curie/Mines ParisTech), Chloé-Agathe Azencott (Institut Curie/Mines ParisTech), Alex Gramfort (LTCI/Télécom ParisTech), Balázs Kégl (LAL/CNRS)_\n",
    "\n",
    "## Introduction\n",
    "This is an initiation project to introduce RAMP and get you to know how it works.\n",
    "\n",
    "The goal is to develop prediction models able to **identify people who survived from the sinking of the Titanic, based on gender, age, and ticketing information**. \n",
    "\n",
    "The data we will manipulate is from the [Titanic kaggle challenge](https://www.kaggle.com/c/titanic-gettingStarted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "* numpy>=1.10.0  \n",
    "* matplotlib>=1.5.0 \n",
    "* pandas>=0.19.0  \n",
    "* scikit-learn>=0.17 (different syntaxes for v0.17 and v0.18)   \n",
    "* seaborn>=0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filename = 'data/train.csv'\n",
    "data = pd.read_csv(train_filename)\n",
    "y_train = data['Survived'].values\n",
    "X_train = data.drop(['Survived', 'PassengerId'], axis=1)\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original training data frame has 891 rows. In the starting kit, we give you a subset of 445 rows. Some passengers have missing information: in particular `Age` and `Cabin` info can be missing. The meaning of the columns is explained on the [challenge website](https://www.kaggle.com/c/titanic-gettingStarted/data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting survival\n",
    "\n",
    "The goal is to predict whether a passenger has survived from other known attributes. Let us group the data according to the `Survived` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Survived').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About two thirds of the passengers perished in the event. A dummy classifier that systematically returns \"0\" would have an accuracy of 62%, higher than that of a random model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features densities and co-evolution\n",
    "A scatterplot matrix allows us to visualize:\n",
    "* on the diagonal, the density estimation for each feature\n",
    "* on each of the off-diagonal plots, a scatterplot between two features. Each dot represents an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(data.get(['Fare', 'Pclass', 'Age']), alpha=0.2,\n",
    "               figsize=(8, 8), diagonal='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linearly transformed data\n",
    "\n",
    "The `Fare` variable has a very heavy tail. We can log-transform it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_plot = data.get(['Age', 'Survived'])\n",
    "data_plot = data.assign(LogFare=lambda x : np.log(x.Fare + 10.))\n",
    "scatter_matrix(data_plot.get(['Age', 'LogFare']), alpha=0.2, figsize=(8, 8), diagonal='kde');\n",
    "\n",
    "data_plot.plot(kind='scatter', x='Age', y='LogFare', c='Survived', s=50, cmap=plt.cm.Paired);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the bivariate distributions and marginals of two variables \n",
    "\n",
    "Another way of visualizing relationships between variables is to plot their bivariate distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.jointplot(x=data_plot.Age[data_plot.Survived == 1],\n",
    "              y=data_plot.LogFare[data_plot.Survived == 1],\n",
    "              kind=\"kde\", size=7, space=0, color=\"b\");\n",
    "\n",
    "sns.jointplot(x=data_plot.Age[data_plot.Survived == 0],\n",
    "              y=data_plot.LogFare[data_plot.Survived == 0],\n",
    "              kind=\"kde\", size=7, space=0, color=\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "A basic prediction workflow, using scikit-learn, will be presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will perform some simple preprocessing of our data:\n",
    "\n",
    "* [one-hot encode](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) the categorical features: `Sex`, `Pclass`, `Embarked`\n",
    "* for the numerical columns `Age`, `SibSp`, `Parch`, `Fare`, fill in missing values with a default value (`-1`)\n",
    "* all remaining columns will be dropped\n",
    "\n",
    "This can be done succintly with [`make_column_transformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html) which performs specific transformations on specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categorical_cols = ['Sex', 'Pclass', 'Embarked']\n",
    "numerical_cols = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    (SimpleImputer(strategy='constant', fill_value=-1), numerical_cols),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocessor` object created with `make_column_transformer` can be used in a scikit-learn [`pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). A `pipeline` assembles several steps together and can be used to cross validate an entire workflow. Generally, transformation steps are combined with a final estimator.\n",
    "\n",
    "We will create a pipeline consisting of the `preprocessor` created above and a final estimator, `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', preprocessor),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cross-validate our `pipeline` using `cross_val_score`. Below we will have specified `cv=8` meaning KFold cross-valdiation splitting will be used, with 8 folds. The Area Under the Receiver Operating Characteristic Curve (ROC AUC) score is calculated for each split. The output `score` will be an array of 8 scores from each KFold. The score mean and standard of the 8 scores is printed at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=8, scoring='roc_auc')\n",
    "\n",
    "print(\"mean: %e (+/- %e)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Once you have created a model with cross-valdiation scores you are happy with, you can test how well your model performs on the independent test data.\n",
    "\n",
    "First we will read in our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_filename = 'data/test.csv'\n",
    "# data = pd.read_csv(test_filename)\n",
    "# y_test = data['Survived'].values\n",
    "# X_test = data.drop(['Survived', 'PassengerId'], axis=1)\n",
    "# X_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to fit our pipeline on our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can predict on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can calculate how well our model performed on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# score = roc_auc_score(y_test, y_pred)\n",
    "# score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAMP submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For submitting to the [RAMP site](http://ramp.studio), you will need to write a `submission.py` file that defines a `get_estimator` function that returns a scikit-learn [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "For example, to submit our basic example above, we would define our `pipeline` within the function and return the pipeline at the end. Remember to include all the necessary imports at the beginning of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_estimator():\n",
    "\n",
    "    categorical_cols = ['Sex', 'Pclass', 'Embarked']\n",
    "    numerical_cols = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        (SimpleImputer(strategy='constant', fill_value=-1), numerical_cols),\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('transformer', preprocessor),\n",
    "        ('classifier', LogisticRegression()),\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at the sample submission in the directory `submissions/starting_kit`, you will find a file named `submission.py`, which has the above code in it.\n",
    "\n",
    "You can test that the sample submission works by running `ramp_test_submission` in your terminal (ensure that `ramp-workflow` has been installed and you are in the `titanic` ramp kit directory). Alternatively, within this notebook you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !ramp_test_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that your own submission works, create a new folder within `submissions` and name it how you wish. Within your new folder save your `submission.py` file that defines a `get_estimator` function. Test your submission locally by running:\n",
    "\n",
    "`ramp_test_submission --submission <folder>`\n",
    "\n",
    "where `<folder>` is the name of the new folder you created above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to [ramp.studio](http://ramp.studio)\n",
    "\n",
    "Once you found a good solution, you can submit it to [ramp.studio](http://www.ramp.studio). First, if it is your first time using RAMP, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then, find the appropriate open event for the [titanic](http://www.ramp.studio/events/titanic) challenge. Sign up for the event. Note that both RAMP and event signups are controlled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request(s) have been accepted, you can go to your [sandbox](http://www.ramp.studio/events/titanic/sandbox) and copy-paste (or upload) your `submissions.py` file. Save your submission, name it, then click 'submit'. The submission is trained and tested on our backend in the same way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](http://www.ramp.studio/events/titanic/my_submissions). Once it is trained, you get a mail, and your submission shows up on the [public leaderboard](http://www.ramp.studio/events/titanic/leaderboard).\n",
    "\n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](http://www.ramp.studio/events/titanic/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credits to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use at the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The usual workflow with RAMP is to explore solutions by refining feature transformations, selecting different models and perhaps do some AutoML/hyperopt, etc., in a notebook setting, then test them with `ramp_test_submission`. The script prints mean cross-validation scores:\n",
    "\n",
    "```\n",
    "----------------------------\n",
    "train auc = 0.85 ± 0.005\n",
    "train acc = 0.81 ± 0.006\n",
    "train nll = 0.45 ± 0.007\n",
    "valid auc = 0.87 ± 0.023\n",
    "valid acc = 0.81 ± 0.02\n",
    "valid nll = 0.44 ± 0.024\n",
    "test auc = 0.83 ± 0.006\n",
    "test acc = 0.76 ± 0.003\n",
    "test nll = 0.5 ± 0.005\n",
    "```\n",
    "\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the [leaderboard](http://www.ramp.studio/events/titanic/leaderboard)) is area under the roc curve (\"auc\"), so the line that is relevant in the output of `ramp_test_submission` is `valid auc = 0.87 ± 0.023`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "Don't hesitate to [contact us](mailto:admin@ramp.studio?subject=titanic notebook)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
